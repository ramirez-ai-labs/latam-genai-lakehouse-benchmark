{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d23545a-a9e2-4f9a-ab2c-de8badc61b53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spanish-Native GenAI Quality Benchmark (Lakehouse Edition)\n",
    "\n",
    "## What is this notebook?\n",
    "\n",
    "This notebook demonstrates a simple, reproducible way to evaluate how language models perform across different Spanish dialects and noisy real-world text.\n",
    "\n",
    "Instead of building a new model, we focus on measuring model performance in realistic conditions before deployment.\n",
    "\n",
    "---\n",
    "\n",
    "## Why does this matter?\n",
    "\n",
    "Most AI systems are evaluated using English benchmarks.\n",
    "\n",
    "However, real users in Latin America:\n",
    "- Use regional slang (El Salvador vs Perú)\n",
    "- Mix formal and informal language\n",
    "- Send noisy text (OCR errors, typos, web copy)\n",
    "\n",
    "If we only test in English, we may miss performance gaps that affect real users.\n",
    "\n",
    "This notebook shows how to measure those differences using Databricks and Delta Lake.\n",
    "\n",
    "---\n",
    "\n",
    "## Lakehouse Architecture (Simple Version)\n",
    "\n",
    "We organize the data into three layers:\n",
    "\n",
    "**Bronze (Raw Data)**\n",
    "- Original dialect examples\n",
    "- Includes region and noise level\n",
    "\n",
    "**Silver (Evaluation Items)**\n",
    "- Structured prompts created from the raw text\n",
    "- Ready to be sent to a model\n",
    "\n",
    "**Gold (Evaluation Results)**\n",
    "- Model outputs and quality metrics\n",
    "- Aggregated performance by region and noise\n",
    "\n",
    "---\n",
    "\n",
    "## What we measure\n",
    "\n",
    "We compare model accuracy across:\n",
    "- Region (SV vs PE)\n",
    "- Noise level (clean vs OCR noise)\n",
    "\n",
    "The goal is to detect performance gaps before production deployment.\n",
    "\n",
    "---\n",
    "\n",
    "This is a simplified demo version of what could become a production-grade GenAI evaluation pipeline. \n",
    "In production systems, this type of evaluation can be used to gate model deployment, detect regressions, and reduce regional bias risk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d2f15bc-4ff8-425d-91c4-99848fa3ef22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Environment Check: Confirm Compute Works\n",
    "# --------------------------------------\n",
    "\n",
    "spark.range(5).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf480ab5-dfa4-4772-9dc6-939535691b44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Bronze Layer: Raw Dialect Data\n",
    "# --------------------------------------\n",
    "\n",
    "data = [\n",
    "    # El Salvador (SV) - clean\n",
    "    (\"SV\", \"Está yuca.\", \"slang\", \"clean\"),\n",
    "    (\"SV\", \"Vaya pues, chero.\", \"slang\", \"clean\"),\n",
    "    (\"SV\", \"¡Puchica! No tengo pisto.\", \"slang\", \"clean\"),\n",
    "    (\"SV\", \"¿Qué onda? Todo chivo.\", \"slang\", \"clean\"),\n",
    "\n",
    "    # Perú (PE) - clean\n",
    "    (\"PE\", \"Ese pata es de mi barrio.\", \"slang\", \"clean\"),\n",
    "    (\"PE\", \"Está bien chévere.\", \"slang\", \"clean\"),\n",
    "    (\"PE\", \"Me paltea un montón.\", \"slang\", \"clean\"),\n",
    "    (\"PE\", \"Ya pues, causa.\", \"slang\", \"clean\"),\n",
    "\n",
    "    # Noise variants (OCR/web-like)\n",
    "    (\"SV\", \"N0 teng0 p1st0\", \"slang\", \"ocr_noise\"),\n",
    "    (\"PE\", \"ESE pata es de mi barri0\", \"slang\", \"ocr_noise\"),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    data,\n",
    "    [\"region\", \"text\", \"task_type\", \"noise_level\"]\n",
    ")\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"latam_bronze_dialect_raw\")\n",
    "\n",
    "spark.table(\"latam_bronze_dialect_raw\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7adbb107-be58-4c3e-bb8c-8f9dab238ef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Silver Layer: Structured Evaluation Items\n",
    "# --------------------------------------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "bronze_df = spark.table(\"latam_bronze_dialect_raw\")\n",
    "\n",
    "silver_df = bronze_df.withColumn(\n",
    "    \"prompt\",\n",
    "    F.concat(F.lit(\"Explain in neutral Spanish: \"), F.col(\"text\"))\n",
    ")\n",
    "\n",
    "silver_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"latam_silver_eval_items\")\n",
    "\n",
    "spark.table(\"latam_silver_eval_items\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "605bc6e1-65a4-485f-a612-2b6587589226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Gold Layer: Model Evaluation Results\n",
    "# --------------------------------------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "silver_df = spark.table(\"latam_silver_eval_items\")\n",
    "\n",
    "gold_df = (\n",
    "    silver_df\n",
    "    .withColumn(\"model_name\", F.lit(\"baseline_model\"))\n",
    "    # base accuracy (reproducible random seed)\n",
    "    .withColumn(\"base_acc\", F.expr(\"0.75 + rand(42) * 0.20\"))\n",
    "    # penalize noisy text\n",
    "    .withColumn(\n",
    "        \"noise_penalty\",\n",
    "        F.when(F.col(\"noise_level\") == \"ocr_noise\", F.lit(0.15))\n",
    "         .otherwise(F.lit(0.0))\n",
    "    )\n",
    "    .withColumn(\"accuracy\", F.col(\"base_acc\") - F.col(\"noise_penalty\"))\n",
    "    .drop(\"base_acc\", \"noise_penalty\")\n",
    ")\n",
    "\n",
    "gold_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"latam_gold_eval_runs\")\n",
    "\n",
    "spark.table(\"latam_gold_eval_runs\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58186ad5-453b-4db7-a58c-3d582e9559e4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771459806743}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Performance Summary: Region × Noise Level\n",
    "# --------------------------------------\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT region,\n",
    "       noise_level,\n",
    "       AVG(accuracy) as avg_accuracy,\n",
    "       COUNT(*) as n_items\n",
    "FROM latam_gold_eval_runs\n",
    "GROUP BY region, noise_level\n",
    "\"\"\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0a015a5-abfb-41db-b9f5-17af89812d2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Gold Summary Table (Dashboard Ready)\n",
    "# --------------------------------------\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE latam_gold_eval_summary AS\n",
    "SELECT region,\n",
    "       noise_level,\n",
    "       AVG(accuracy) as avg_accuracy,\n",
    "       COUNT(*) as n_items\n",
    "FROM latam_gold_eval_runs\n",
    "GROUP BY region, noise_level\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_latam_benchmark_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
